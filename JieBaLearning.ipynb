{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 我/ 在/ 学习/ 自然/ 自然语言/ 语言\n",
      "我, 是, 曹凯强\n",
      "['曹凯强', '喜欢', '陈盼', '盼']\n",
      "如果/放到/旧/字典/中/将/出错/。\n"
     ]
    }
   ],
   "source": [
    "#encoding=utf-8\n",
    "import jieba\n",
    "seq_list = jieba.cut(\"我在学习自然语言\",cut_all = True);\n",
    "print(\"Full Mode: \" + \"/ \".join(seq_list))  # 全模式\n",
    "seg_list = jieba.cut_for_search(\"我是曹凯强\")  # 搜索引擎模式\n",
    "print(\", \".join(seg_list))\n",
    "result_lcut = jieba.lcut(\"曹凯强喜欢陈盼盼\")\n",
    "print (result_lcut)\n",
    "jieba.suggest_freq(('中','将'),True);\n",
    "print('/'.join(jieba.cut('如果放到旧字典中将出错。', HMM=False)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "韦少 杜兰特 全明星 全明星赛 MVP 威少 正赛 科尔 投篮 勇士 球员 斯布鲁克 更衣柜 NBA 三连庄 张卫平 西部 指导 雷霆 明星队\n"
     ]
    }
   ],
   "source": [
    "#encoding=utf-8\n",
    "import jieba.analyse as analyse\n",
    "lines = open(\"/Users/hcnucai/Documents/KevinSwift/机器学习资料/02-自然语言处理班视频教程附讲义代码/课件资料/Lecture_1/NBA.txt\").read()\n",
    "print (\" \".join(analyse.extract_tags(lines,topK = 20,withWeight = False,allowPOS = ())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "陈盼盼,nr\n",
      "是,v\n",
      "一只,m\n",
      "熊猫,nr\n"
     ]
    }
   ],
   "source": [
    "#词性标注\n",
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(\"陈盼盼是一只熊猫\");\n",
    "for word,flag in words:\n",
    "    print('%s,%s' %(word,flag));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "并行分词速度为 68703.19774929665 bytes/second\n",
      "非并行分词速度为 102503.66718487657 bytes/second\n"
     ]
    }
   ],
   "source": [
    "#并行分词\n",
    "import sys;\n",
    "import time;\n",
    "import jieba;\n",
    "#开启并行进程\n",
    "jieba.enable_parallel();\n",
    "content = open(u'/Users/hcnucai/Documents/KevinSwift/机器学习资料/02-自然语言处理班视频教程附讲义代码/课件资料/Lecture_1/NBA.txt',\"r\").read();\n",
    "t1 = time.time();\n",
    "words = \"/ \".join(jieba.cut(content));\n",
    "t2 = time.time();\n",
    "tm_cost = t2 - t1;\n",
    "print('并行分词速度为 %s bytes/second' % (len(content)/tm_cost))\n",
    "jieba.disable_parallel()\n",
    "content = open(u'/Users/hcnucai/Documents/KevinSwift/机器学习资料/02-自然语言处理班视频教程附讲义代码/课件资料/Lecture_1/NBA.txt',\"r\").read()\n",
    "t1 = time.time()\n",
    "words = \"/ \".join(jieba.cut(content))\n",
    "t2 = time.time()\n",
    "tm_cost = t2-t1\n",
    "print('非并行分词速度为 %s bytes/second' % (len(content)/tm_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自然语言\t\t start: 0 \t\t end:4\n",
      "处理\t\t start: 4 \t\t end:6\n",
      "很\t\t start: 6 \t\t end:7\n",
      "有用\t\t start: 7 \t\t end:9\n",
      "\n",
      "-----------我是神奇的分割线------------\n",
      "\n",
      "自然\t\t start: 0 \t\t end:2\n",
      "语言\t\t start: 2 \t\t end:4\n",
      "自然语言\t\t start: 0 \t\t end:4\n",
      "处理\t\t start: 4 \t\t end:6\n",
      "很\t\t start: 6 \t\t end:7\n",
      "有用\t\t start: 7 \t\t end:9\n"
     ]
    }
   ],
   "source": [
    "#返回词语在原文的起止位置\n",
    "result = jieba.tokenize(u'自然语言处理很有用');\n",
    "for tk in result:\n",
    "     print(\"%s\\t\\t start: %d \\t\\t end:%d\" % (tk[0],tk[1],tk[2]))\n",
    "print (\"\\n-----------我是神奇的分割线------------\\n\");\n",
    "result = jieba.tokenize(u'自然语言处理很有用',mode = 'search');\n",
    "for tk in result:\n",
    "    print(\"%s\\t\\t start: %d \\t\\t end:%d\" % (tk[0],tk[1],tk[2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'whoosh'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c68f0a820bb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#添加路径\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwhoosh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopen_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwhoosh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwhoosh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqparser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQueryParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'whoosh'"
     ]
    }
   ],
   "source": [
    "#将模块中显示出现的所有字符转为unicode类型 字符类型 对于str都是字节类型的\n",
    "from __future__ import unicode_literals\n",
    "#添加路径\n",
    "sys.path.append(\"../\")\n",
    "from whoosh.index import create_in,open_dir\n",
    "from whoosh.fields import *\n",
    "from whoosh.qparser import QueryParser\n",
    "analyzer = jieba.analyse.ChineseAnalyzer();\n",
    "schema = Schema(title=TEXT(stored=True), path=ID(stored=True), content=TEXT(stored=True, analyzer=analyzer))\n",
    "    \n",
    "if not os.path.exists(\"tmp\"):\n",
    "    os.mkdir(\"tmp\")\n",
    "\n",
    "ix = create_in(\"tmp\", schema) # for create new index\n",
    "#ix = open_dir(\"tmp\") # for read only\n",
    "writer = ix.writer()\n",
    "writer.add_document(\n",
    "title = 'document1',\n",
    "path = \"/a\",\n",
    "content = \"this is first\"\n",
    ")\n",
    "writer.commit();\n",
    "searcher = ix.searcher()\n",
    "parser = QueryParser(\"content\", schema=ix.schema)\n",
    "\n",
    "for keyword in (\"水果世博园\",\"你\",\"first\",\"中文\",\"交换机\",\"交换\"):\n",
    "    print(keyword+\"的结果为如下：\")\n",
    "    q = parser.parse(keyword)\n",
    "    results = searcher.search(q)\n",
    "    for hit in results:\n",
    "        print(hit.highlights(\"content\"))\n",
    "    print(\"\\n--------------我是神奇的分割线--------------\\n\")\n",
    "\n",
    "for t in analyzer(\"我的好朋友是李明;我爱北京天安门;IBM和Microsoft; I have a dream. this is intetesting and interested me a lot\"):\n",
    "    print(t.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
