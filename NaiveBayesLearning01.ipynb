{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1 december wereld aids dag voorlichting in zuidafrika over bieten taboes en optimisme', 'nl'), ('1 millón de afectados ante las inundaciones en sri lanka unicef está distribuyendo ayuda de emergencia srilanka', 'es'), ('1 millón de fans en facebook antes del 14 de febrero y paty miki dani y berta se tiran en paracaídas qué harías tú porunmillondefans', 'es'), ('1 satellite galileo sottoposto ai test presso lesaestec nl galileo navigation space in inglese', 'it'), ('10 der welt sind bei', 'de')]\n"
     ]
    }
   ],
   "source": [
    "#encoding=utf-8\n",
    "#语种检测\n",
    "import jieba\n",
    "in_f = open(\"/Users/hcnucai/Documents/KevinSwift/机器学习资料/02-自然语言处理班视频教程附讲义代码/课件资料/Lecture_2/Language-Detector/data.csv\");\n",
    "lines = in_f.readlines();\n",
    "in_f.close();\n",
    "dataset = [(line.strip()[:-3],line.strip()[-2:]) for line in lines];\n",
    "#输出数据\n",
    "print (dataset[:5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把原始数据分为训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6799\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#将对象打包成一个个的元组 利用*解压成列表\n",
    "x,y = zip(*dataset); #反解压\n",
    "#random_state：是随机数的种子。\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 1);\n",
    "print (len(x_train));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trump images are now more popular than cat gifs.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#去掉噪声数据\n",
    "import re\n",
    "def remove_noise(document):\n",
    "    noise_pattern = re.compile(\"|\".join([\"http\\S+\",\"\\@\\w+\", \"\\#\\w+\"]));\n",
    "    clean_text = re.sub(noise_pattern,\"\",document);\n",
    "    return clean_text.strip();\n",
    "remove_noise(\"Trump images are now more popular than cat gifs. @trump #trends http://www.trumptrends.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抽取降噪后数据上的特征 采用1-gram 和 2-gram #搜索关键词\n",
    "#将文本数据转化为特征向量的过程 比较常用的有词袋法 CountVectorizer只考虑词汇在文本中出现的频率\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(\n",
    "    lowercase = True,\n",
    "    analyzer = 'char_wb', \n",
    "    ngram_range = (1,2),\n",
    "    max_features = 1000, #最多的特征数\n",
    "    preprocessor = remove_noise #降噪的\n",
    ");\n",
    "def get_features(x):\n",
    "    vec.transform(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把贝叶斯import 进来进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB;\n",
    "classifier = MultinomialNB();\n",
    "classifier.fit(vec.transform(x_train),y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9770621967357741\n"
     ]
    }
   ],
   "source": [
    "print (classifier.score(vec.transform(x_test),y_test));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it']\n",
      "0.9770621967357741\n"
     ]
    }
   ],
   "source": [
    "#规范化 写成一个class\n",
    "import re;\n",
    "from sklearn.feature_extraction.text import CountVectorizer;\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "class LanguageDetector():\n",
    "   \n",
    "    def __init__(self, classifier=MultinomialNB()):\n",
    "        self.classifier = classifier\n",
    "        self.vectorizer = CountVectorizer(ngram_range=(1,2), max_features=1000, preprocessor=self._remove_noise)\n",
    "\n",
    "    def _remove_noise(self, document):\n",
    "        noise_pattern = re.compile(\"|\".join([\"http\\S+\", \"\\@\\w+\", \"\\#\\w+\"]))\n",
    "        clean_text = re.sub(noise_pattern, \"\", document)\n",
    "        return clean_text\n",
    "\n",
    "    def features(self, X):\n",
    "        return self.vectorizer.transform(X)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.vectorizer.fit(X)\n",
    "        self.classifier.fit(self.features(X), y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.classifier.predict(self.features([x]))\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.classifier.score(self.features(X), y)\n",
    "in_f = open(\"/Users/hcnucai/Documents/KevinSwift/机器学习资料/02-自然语言处理班视频教程附讲义代码/课件资料/Lecture_2/Language-Detector/data.csv\");\n",
    "lines = in_f.readlines()\n",
    "in_f.close()\n",
    "dataset = [(line.strip()[:-3], line.strip()[-2:]) for line in lines]\n",
    "x, y = zip(*dataset)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1)\n",
    "\n",
    "language_detector = LanguageDetector()\n",
    "language_detector.fit(x_train, y_train)\n",
    "print(language_detector.predict('algunas cuentas del f煤tbol europeo y'))\n",
    "print(language_detector.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
