{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/hcnucai/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "import sys\n",
    "from nltk.corpus import brown\n",
    "brown_tags_words = [ ]\n",
    "for sent in brown.tagged_sents():\n",
    "    # 先加开头\n",
    "    brown_tags_words.append( (\"START\", \"START\") )\n",
    "    # 为了省事儿，我们把tag都省略成前两个字母\n",
    "    brown_tags_words.extend([ (tag[:2], word) for (word, tag) in sent ])\n",
    "    # 加个结尾\n",
    "    brown_tags_words.append( (\"END\", \"END\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of an adjective (JJ) being 'new' is 0.01472344917632025\n",
      "The probability of a verb (VB) being 'duck' is 6.042713350943527e-05\n"
     ]
    }
   ],
   "source": [
    "#词统计\n",
    "cfd_tagwords = nltk.ConditionalFreqDist(brown_tags_words);\n",
    "cpd_tagwords = nltk.ConditionalProbDist(cfd_tagwords,nltk.MLEProbDist);\n",
    "#查看统计结果\n",
    "print (\"The probability of an adjective (JJ) being 'new' is\", cpd_tagwords[\"JJ\"].prob(\"new\"));\n",
    "print(\"The probability of a verb (VB) being 'duck' is\", cpd_tagwords[\"VB\"].prob(\"duck\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we have just seen 'DT', the probability of 'NN' is 0.5057722522030194\n",
      "If we have just seen 'VB', the probability of 'JJ' is 0.016885067592065053\n",
      "If we have just seen 'VB', the probability of 'NN' is 0.10970977711020183\n"
     ]
    }
   ],
   "source": [
    "brown_tags = [tag for (tag, word) in brown_tags_words ]\n",
    "# count(t{i-1} ti)\n",
    "# bigram的意思是 前后两个一组，联在一起\n",
    "cfd_tags= nltk.ConditionalFreqDist(nltk.bigrams(brown_tags))\n",
    "# P(ti | t{i-1})\n",
    "cpd_tags = nltk.ConditionalProbDist(cfd_tags, nltk.MLEProbDist)\n",
    "print(\"If we have just seen 'DT', the probability of 'NN' is\", cpd_tags[\"DT\"].prob(\"NN\"))\n",
    "print( \"If we have just seen 'VB', the probability of 'JJ' is\", cpd_tags[\"VB\"].prob(\"DT\"))\n",
    "print( \"If we have just seen 'VB', the probability of 'NN' is\", cpd_tags[\"VB\"].prob(\"NN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of the tag sequence 'START PP VB TO VB END' for 'I want to race' is: 1.0817766461150474e-14\n"
     ]
    }
   ],
   "source": [
    "prob_tagsequence = cpd_tags[\"START\"].prob(\"PP\") * cpd_tagwords[\"PP\"].prob(\"I\") * \\\n",
    "    cpd_tags[\"PP\"].prob(\"VB\") * cpd_tagwords[\"VB\"].prob(\"want\") * \\\n",
    "    cpd_tags[\"VB\"].prob(\"TO\") * cpd_tagwords[\"TO\"].prob(\"to\") * \\\n",
    "    cpd_tags[\"TO\"].prob(\"VB\") * cpd_tagwords[\"VB\"].prob(\"race\") * \\\n",
    "    cpd_tags[\"VB\"].prob(\"END\")\n",
    "\n",
    "print( \"The probability of the tag sequence 'START PP VB TO VB END' for 'I want to race' is:\", prob_tagsequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viterbi的实现\n",
    "distinct_tags = set(brown_tags);\n",
    "sentence = [\"I\",\"want\",\"to\",\"race\"];\n",
    "senlen = len(sentence);\n",
    "#开始维特比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{':-': 0.0, '``': 0.0, '.': 0.0, 'WD': 0.0, 'NN': 1.0580313619573935e-06, 'AP': 0.0, '(-': 0.0, ')': 0.0, \"''\": 0.0, 'END': 0.0, ':': 0.0, 'PN': 0.0, '*-': 0.0, 'PP': 0.014930900689060006, 'FW': 0.0, 'DT': 0.0, ')-': 0.0, 'WQ': 0.0, 'IN': 0.0, 'AT': 0.0, 'NP': 1.7319067623793952e-06, 'VB': 0.0, 'OD': 0.0, 'HV': 0.0, '(': 0.0, 'RB': 0.0, ',-': 0.0, 'RP': 0.0, ',': 0.0, 'QL': 0.0, 'BE': 0.0, 'CS': 0.0, '--': 0.0, 'CD': 0.0, 'UH': 0.0, 'CC': 0.0, 'NR': 0.0, 'TO': 0.0, 'DO': 0.0, 'AB': 0.0, 'JJ': 0.0, 'MD': 0.0, 'WP': 0.0, \"'\": 0.0, 'NI': 3.3324520848931064e-07, '*': 0.0, 'WR': 0.0, '.-': 0.0, 'EX': 0.0, 'RN': 0.0}\n",
      "{':-': 'START', '``': 'START', '.': 'START', 'WD': 'START', 'NN': 'START', 'AP': 'START', '(-': 'START', ')': 'START', \"''\": 'START', 'END': 'START', ':': 'START', 'PN': 'START', '*-': 'START', 'PP': 'START', 'FW': 'START', 'DT': 'START', ')-': 'START', 'WQ': 'START', 'IN': 'START', 'AT': 'START', 'NP': 'START', 'VB': 'START', 'OD': 'START', 'HV': 'START', '(': 'START', 'RB': 'START', ',-': 'START', 'RP': 'START', ',': 'START', 'QL': 'START', 'BE': 'START', 'CS': 'START', '--': 'START', 'CD': 'START', 'UH': 'START', 'CC': 'START', 'NR': 'START', 'TO': 'START', 'DO': 'START', 'AB': 'START', 'JJ': 'START', 'MD': 'START', 'WP': 'START', \"'\": 'START', 'NI': 'START', '*': 'START', 'WR': 'START', '.-': 'START', 'EX': 'START', 'RN': 'START'}\n"
     ]
    }
   ],
   "source": [
    "viterbi = [];\n",
    "backpointer = [];\n",
    "first_viterbi = {};\n",
    "first_backpointer = {};\n",
    "for tag in distinct_tags:\n",
    "    if tag == 'START':\n",
    "        continue;\n",
    "    first_viterbi[ tag ] = cpd_tags[\"START\"].prob(tag) * cpd_tagwords[tag].prob( sentence[0] )\n",
    "    first_backpointer[ tag ] = \"START\"\n",
    "\n",
    "print(first_viterbi)\n",
    "print(first_backpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'I' current best two-tag sequence: START PP\n"
     ]
    }
   ],
   "source": [
    "viterbi.append(first_viterbi);\n",
    "backpointer.append(first_backpointer);\n",
    "currbest = max(first_viterbi.keys(), key = lambda tag: first_viterbi[ tag ])\n",
    "print( \"Word\", \"'\" + sentence[0] + \"'\", \"current best two-tag sequence:\", first_backpointer[ currbest], currbest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'want' current best two-tag sequence: PP VB\n",
      "Word 'to' current best two-tag sequence: VB TO\n",
      "Word 'race' current best two-tag sequence: IN NN\n"
     ]
    }
   ],
   "source": [
    "#开始循环\n",
    "for wordindex in range(1, len(sentence)):\n",
    "    this_viterbi = { }\n",
    "    this_backpointer = { }\n",
    "    prev_viterbi = viterbi[-1]\n",
    "    \n",
    "    for tag in distinct_tags:\n",
    "        # START没有卵用的，我们要忽略\n",
    "        if tag == \"START\": continue\n",
    "        \n",
    "        # 如果现在这个tag是X，现在的单词是w，\n",
    "        # 我们想找前一个tag Y，并且让最好的tag sequence以Y X结尾。\n",
    "        # 也就是说\n",
    "        # Y要能最大化：\n",
    "        # prev_viterbi[ Y ] * P(X | Y) * P( w | X)\n",
    "        \n",
    "        best_previous = max(prev_viterbi.keys(),\n",
    "                            key = lambda prevtag: \\\n",
    "            prev_viterbi[ prevtag ] * cpd_tags[prevtag].prob(tag) * cpd_tagwords[tag].prob(sentence[wordindex]))\n",
    "\n",
    "        this_viterbi[ tag ] = prev_viterbi[ best_previous] * \\\n",
    "            cpd_tags[ best_previous ].prob(tag) * cpd_tagwords[ tag].prob(sentence[wordindex])\n",
    "        this_backpointer[ tag ] = best_previous\n",
    "    \n",
    "    # 每次找完Y 我们把目前最好的 存一下\n",
    "    currbest = max(this_viterbi.keys(), key = lambda tag: this_viterbi[ tag ])\n",
    "    print( \"Word\", \"'\" + sentence[ wordindex] + \"'\", \"current best two-tag sequence:\", this_backpointer[ currbest], currbest)\n",
    "\n",
    "\n",
    "    # 完结\n",
    "    # 全部存下来\n",
    "    viterbi.append(this_viterbi)\n",
    "    backpointer.append(this_backpointer)\n",
    "\n",
    "\n",
    "# 找所有以END结尾的tag sequence\n",
    "prev_viterbi = viterbi[-1]\n",
    "best_previous = max(prev_viterbi.keys(),\n",
    "                    key = lambda prevtag: prev_viterbi[ prevtag ] * cpd_tags[prevtag].prob(\"END\"))\n",
    "\n",
    "prob_tagsequence = prev_viterbi[ best_previous ] * cpd_tags[ best_previous].prob(\"END\")\n",
    "\n",
    "# 我们这会儿是倒着存的。。。。因为。。好的在后面\n",
    "best_tagsequence = [ \"END\", best_previous ]\n",
    "# 同理 这里也有倒过来\n",
    "backpointer.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence was: I want to race \n",
      "\n",
      "The best tag sequence is: START PP VB IN NN END \n",
      "\n",
      "The probability of the best tag sequence is: 5.71772824864617e-14\n"
     ]
    }
   ],
   "source": [
    "current_best_tag = best_previous\n",
    "for bp in backpointer:\n",
    "    best_tagsequence.append(bp[current_best_tag])\n",
    "    current_best_tag = bp[current_best_tag]\n",
    "best_tagsequence.reverse()\n",
    "print( \"The sentence was:\", end = \" \")\n",
    "for w in sentence: print( w, end = \" \")\n",
    "print(\"\\n\")\n",
    "print( \"The best tag sequence is:\", end = \" \")\n",
    "for t in best_tagsequence: print (t, end = \" \")\n",
    "print(\"\\n\")\n",
    "print( \"The probability of the best tag sequence is:\", prob_tagsequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
